{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 554 Lab 2: Supervised Classification\n",
    "\n",
    "## Logistic Regression and Gradient Descent\n",
    "\n",
    "In linear regression, we got a real-valued response\n",
    "\t\n",
    "$$y = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_d x_d$$\n",
    "    \n",
    "i.e., a linear combination of inputs, where $y \\in \\mathbf{R}$. \n",
    "\n",
    "In classification, we want an *indication* of how likely an instance is to belong to a particular class; a probability $\\in [0,1]$.  \n",
    "\n",
    "Given a real valued $a$, we can squish it to range $\\sigma(a) \\in [0,1]$ by feeding it through the **logistic function** aka **sigmoid function**:\n",
    "\t\t\n",
    "\\begin{equation}\n",
    "\\sigma(a) = \\frac{1}{1+\\exp(-a)} \\hspace{1.5cm}(1)\n",
    "\\end{equation}\n",
    "\n",
    "Which looks like this: ![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
    "\n",
    "Therefore, we can treat this as a probability, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "P(y=1|\\mathbf{x}) &= \\sigma(\\mathbf{\\theta}^\\top\\mathbf{x}) \\\\\n",
    "P(y=0|\\mathbf{x}) &= 1 - \\sigma(\\mathbf{\\theta}^\\top\\mathbf{x}) \n",
    "\\end{align*}\n",
    "\n",
    "where we omit the bias term and suppose that both $\\mathbf{\\theta}$ and $\\mathbf{x}$ are column vectors.\n",
    "\n",
    "> **Task 1**: implement the sigmoid function (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # insert here the code for Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model, we want weights (i.e., parameters) $\\mathbf{\\theta}$ to reduce the error.\n",
    "It means that the **likelihood** of the whole dataset can be written as: \n",
    "\n",
    "\\begin{equation}\n",
    "P\\left(\\{\\mathbf{x}_i,y_i\\}_{i=1}^n\\right) = \\prod_{i=1}^n P(Y = y_i|\\mathbf{X} = \\mathbf{x}_i) \\hspace{1.5cm}(2)\n",
    "\\end{equation}\n",
    "\n",
    "> **Task 2**: implement the cost function (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, X, y):\n",
    "    # Computes the cost using w as the parameters for logistic regression.\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in OLS of the previous lab, and machine learning in general, we want to find values of the parameters which minimize our cost function. In this case the cost function is the negative log likelihood. We'll call this $E(\\mathbf{ \\theta})$. \n",
    "\n",
    ">**Ques 1**:Derive the gradient of the cost function with respect to the model's parameters $\\nabla_{\\mathbf{\\theta}} E(\\mathbf{\\theta})$.  Hint 1: Recall you should take the *log* likelihood. Hint 2: Prior to taking the log, you an use the expression of a Bernoulli distribution, $\\sigma_i^{y_i} (1-\\sigma_i)^{1-y_i}$ where $\\sigma_i \\equiv \\sigma(\\mathbf{\\theta}^\\top\\mathbf{x}_i)$. Hint 3: A useful derivative $\\sigma' = (1 - \\sigma)\\sigma$.}\n",
    "\n",
    "You'll notice that, unlike under OLS, it's not a closed form expression; you have $\\mathbf{\\theta}$ on the right hand side of the equation. Nevertheless, we can use numerical methods to find optimal ${\\mathbf{\\theta}^*}$.\n",
    "\n",
    ">**Task 3**: Implement the gradient of the cost function (which you just derived above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(w, X, y):\n",
    "    # Computes the gradient of the cost with respect to the parameters.\n",
    "    \n",
    "    dE = np.zeros_like(w) # initialize gradient\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return dE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now is a vector $\\mathbf{g} = \\nabla_{\\mathbf{\\theta}} E(\\mathbf{\\theta})$; the **gradient** of the function. Note that the dimensionality is the same as the input. We can move in the direction of the gradient and thus descend the function. This is the idea of **Gradient Descent**: we iteratively follow the gradient down the error surface. We repeatedly carry out \n",
    "\\begin{equation}\n",
    "\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_{t} - \\alpha \\nabla_{\\mathbf{\\theta}} E(\\mathbf{\\theta}_t) \\hspace{1.5cm}(3)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the **learning rate**; for $t=1,2,\\ldots,T$, such that $\\mathbf{\\theta}^* \\gets \\mathbf{\\theta}_T$.\n",
    "\n",
    "> **Task 4**: Use the functions implemented so far to implement gradient descent, Eq.(3), for a fixed number of steps (say, $T=500\\,000$) and learning rate to (say, $\\alpha=0.005$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "T = 500000\n",
    "\n",
    "for i in range(T): \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 5**: Implement a function to provide predictions $\\hat{\\mathbf{y}} \\in \\{0,1\\}$ for any given $\\mathbf{x}$ and $\\hat{\\mathbf{\\theta}}$ (from the previous task) by assigning $\\hat{\\mathbf{y}}_i = 1$ whenever $\\sigma_i \\geq 0.5$. In other words, you turn logistic regression into a **classifier**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "    # Predict whether each label is 0 or 1 using learned logistic regression parameters w. The threshold is set at 0.5\n",
    "\n",
    "    N = X.shape[0] # number of examples\n",
    "    yp = np.zeros(N) # predicted classes of examples\n",
    "    \n",
    "    # TODO \n",
    "    return yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is data provided in the *data/* folder. Suppose that each column represents an exam score during a course, with the class label indicated *admitted* or not into a Masters program at some university. We want a model to give the probability that a student will be admitted based on the two grades of two courses. Some plotting code is already provided. \n",
    " \n",
    "> **Task 6**: Make a train-test split of the data provided and evaluate your classifier (similarly to as in the first lab). Output and/or plot the error $E(\\mathbf{\\theta}_t)$ both on the training **and** test set, for $t=1,\\ldots,T$. Hint: You may want to plot the average of a moving window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = np.loadtxt('./data/data.csv', delimiter=',')\n",
    " \n",
    "#Add intercept term \n",
    "data_1 = np.ones((data.shape[0], 4))\n",
    "data_1[:, 1:] = data\n",
    "\n",
    "# Standardize the data\n",
    "# (It will still work without standardization, but may behave erratically)\n",
    "data_1[:,1:3] = (data_1[:,1:3] - np.mean(data_1[:,1:3],axis=0)) / np.std(data_1[:,1:3],axis=0)\n",
    "\n",
    "X = data_1[:, 0:3]\n",
    "y = data_1[:, -1]\n",
    "\n",
    "# Plot data \n",
    "pos = np.where(y == 1) # instances of class 1\n",
    "neg = np.where(y == 0) # instances of class 0\n",
    "plt.scatter(X[pos, 1], X[pos, 2], marker='o', c='b')\n",
    "plt.scatter(X[neg, 1], X[neg, 2], marker='x', c='r')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.legend(['Admitted', 'Not Admitted'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "N = X.shape[0]\n",
    "\n",
    "# Initialize fitting parameters \n",
    "w = np.random.randn(3,1) * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ques 2**: Knowing that the cost function is convex, are you convinced that with a suitable learning rate, gradient descent will always converge to the minimum? What are some examples of poor learning rates?\n",
    "\n",
    "> **Task 7**: Following your considerations in the previous question -- can you find a better learning rate, such that you reduce error more efficiently (for smaller $T$?).\n",
    "\n",
    "> **Task 8**: (Bonus) Instead of the whole training set, select a random subset (i.e., minibatch) of 10 examples for each iteration. This is known as **stochastic gradient descent**.\n",
    "\n",
    "> **Ques 3**: (Bonus) What are the advantages of stochastic gradient descent over classical gradient descent? (Especially with a non-convex loss function).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours\n",
    "\n",
    "The method $k$NN takes a different approach to modeling $P(Y=1|\\mathbf{x})$. It is a non-parametric lazy method, and does not have a \"model\" as such. It predicts with\n",
    "$$\n",
    "P(Y=1|\\mathbf{x}) \\approx \\frac{1}{k} \\sum_{\\mathbf{x}_i \\in \\textsf{Ne}_k(\\mathbf{x})} y_i \n",
    "$$\n",
    "where $\\textsf{Ne}_k(\\mathbf{x})$ is the *neighbourhood* of the $k$ training examples closest to $\\mathbf{x}$ (typically measured by Euclidean distance). \n",
    "\n",
    "> **Ques 4**: What is the effect of different values of $k$? Hint: Draw on paper and/or empirically test different values.\n",
    "\n",
    "> **Ques 5**: What is the complexity of making a prediction with a naive implementation of $k$-NN? When is this likely to be a problem in practice?\n",
    "\n",
    "> **Task 9**: (Bonus) Implement this function to create a $k$-NN classifier and evaluate it on the data provided. Hint: there is not really any training stage here; simple store the training instances to search over later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def kNN(k, X, y, x):\n",
    "    '''\n",
    "    kNN classification of x\n",
    "    -----------------------\n",
    "        Input: \n",
    "        k: number of nearest neighbors\n",
    "        X: training data           \n",
    "        y: class labels of training data\n",
    "        x: test instance\n",
    "\n",
    "        return the label to be associated with x\n",
    "\n",
    "        Hint: you may use the function 'norm' \n",
    "    '''\n",
    "    # Enter here your solution for Task 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
