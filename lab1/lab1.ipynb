{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 554 Lab 1: introduction to the machine learning pipeline\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this lab is to demonstrate the *machine learning pipeline*. We cover a step by step overview of a typical machine learning task, and at each step describe the subtasks that need to be performed. For further information regarding the concepts relevant to this lab, see the lecture slides.\n",
    "\n",
    "## The Task and Data\n",
    "\n",
    "Data is available in the ```data``` folder. This data consists of rows of measurements associated with cell growth in Scots pine trees monitored in the commune of Walscheid in France. Each row corresponds to the data of one week. The features (we shall denote by variables $X_1,\\ldots,X_3$) are: the week number, the average measured temperature, and soil moisture over that week. The final column (we shall denote $Y$) is the target; the number of new cells (known as \\textit{tracheids}) measured during the corresponding week (the number is an average over measurements from several trees). Let $\\mathbf{X} \\in \\mathbf{R}^{n \\times 3}$ be the instances ($n$ weeks; i.e., each row $\\mathbf{x}_i$ is an instance; corresponding to realizations of $X_1,\\ldots,X_3$, associated with label $y_i$); and $\\mathbf{y} = [y_1,\\ldots,y_n]^\\top$ are the labels. \n",
    "\n",
    "Counting cells involves manually extracting micro-core samples from the tree, and counting the cells under a microscope. Therefore it would be time saving and beneficial if a computational model was constructed, such that an estimate of growth could be made automatically given the environmental measurements such as temperature and daylight hours (which are easily and automatically obtainable), i.e., a data-driven model. Furthermore, the model could be analyzed for greater understanding of growth drivers, and we could also build a forecasting model.\n",
    "\n",
    "## Implementation of a Machine Learning Pipeline\n",
    "\n",
    "In this section, a machine learning pipeline will be implemented to load and preprocess the data, and from this data to build and evaluate a regression model. Each of the following subsections include tasks to be completed. In each case, you will need to complete the code in the Notebook cells where requested. The data is contained in the ```data``` folder. \n",
    "\n",
    "## Inspecting the data\n",
    "\n",
    "The training data is loaded and stored in ```X``` and ```y```, respectively. The distribution of each variable can be inspected by, for example, using ```Matplotlib```'s ```hist``` function. Notice/recall that in Python the first feature is indexed at ```0```, whereas we refer to $X_1$ in mathematical notation. Therefore the second feature is indexed at ```1```, and so on. Execute the next cell and observe the result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6.,  4., 15., 17., 25., 15., 12., 26., 22., 17.]),\n",
       " array([-4.89 , -2.282,  0.326,  2.934,  5.542,  8.15 , 10.758, 13.366,\n",
       "        15.974, 18.582, 21.19 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "data = np.loadtxt('data/data_train.csv', delimiter=',', dtype=np.dtype(\"float64\"))\n",
    "\n",
    "# Prepare the data\n",
    "\n",
    "X = data[:,0:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "# Inspect the data\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(X[:,1], 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 1:** Build the histogram for all other variables, then also plot $X_1$ (week number) versus the target (number of cells).\n",
    "\n",
    "(The next cell shows the plot of $x_3$ versus $x_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$x_3$')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(X[:,1],X[:,2], 'o')\n",
    "plt.xlabel('$x_2$')\n",
    "plt.ylabel('$x_3$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between $X_1$ (week number) and the target variable (number of cells) should show how the growth season starts at about the 12th week of the year, peaks in the summer months, before ceasing in the final months of the year.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "Preprocessing is a fundamental step in the machine learning pipeline. In practice, it takes up most of the time. It can involve cleaning the data, dealing with missing values, removing outliers, dimensionality reduction, feature selection and feature engineering. Usually preprocessing is guided by the data exploration process (inspecting the data). For example, there appears to be at outlier in the target column, possibly caused by an $8$ being recorded instead of a $0$. We will come back to this later at evaluation time.\n",
    "\n",
    "In the data exploration phase it was also noticeable that the attributes were of different scales. A common technique is to standardize each attribute to mean $0$ and standard deviation $1$ so that each variable will be considered equally.\n",
    "\n",
    "> **Task 2:** Standardize the data of each of the input attributes. Hint: NumPy has a ```mean``` function. Calling ```mean(X,axis=0)``` will return a vector of means, one for each column. The function ```std``` can be used in a similar way for the standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26.   , 10.23 ,  0.358])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X, axis=0) #you should obtain array([26.   , 10.23 ,  0.358])\n",
    "\n",
    "#insert here your solution for Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model\n",
    "\n",
    "A standard approach for regression tasks (such as this one, where the target label is continuous) is *ordinary least squares* (OLS). However, OLS is a linear model, yet we observed previously that the relationship of the $X_1$ feature is non-linear with respect to $Y$. To turn a linear model into a non-linear one, we can use polynomial *basis functions* to create a new feature space, such that by fitting a linear model to this new feature space we obtain a non-linear model. This has been included in the preprocessing\\footnote{It is a matter of perspective whether this step is considered part of building the model, or data preprocessing} via the function ```phi```, where the second parameter indicates the degree to the polynomial. Using degree $2$ would produce a new feature space\n",
    "\n",
    "$$\n",
    "\t\\mathbf{z} = \\phi(\\mathbf{x}) = [1,x_1,x_2,x_3,x^2_1,x^2_2,x^2_3]\n",
    "$$\n",
    "\n",
    "for an instance $\\mathbf{x}$. For degree $1$, this is equivalent to the original feature space,\n",
    "\n",
    "$$\n",
    "\t\\mathbf{z} = \\phi(\\mathbf{x}) = [1,x_1,x_2,x_3]\n",
    "$$\n",
    "\n",
    "except that we have added a column of $1$s to the new input feature space. This is common practice so that the intercept/bias does not need to be calculated separately.\n",
    "\n",
    "Our model is represented by the best set of coefficients\n",
    "\n",
    "$$\n",
    "\t\\mathbf{\\hat\\beta} = [\\beta_0,\\beta_1,\\ldots,\\beta_m]^\\top\n",
    "$$\n",
    "\n",
    "where $m$ is the length of vector $\\mathbf{z}$. And where $\\mathbf{\\hat\\beta}$ refers to the values which minimize the mean squared error (or equivalently, sum of squares error) over the data; which is our given/assumed loss metric; given in Eq. (2) (where it is later used for evaluation). Having $\\mathbf{\\hat\\beta}$, we can now make a prediction for any test point;\n",
    "\n",
    "$$\n",
    "\t\\hat{y} = \\mathbf{z\\hat\\beta}\n",
    "$$\n",
    "\n",
    "> **Ques 1**: Derive $\\mathbf{\\hat\\beta}$, by minimizing Eq. (2); setting it to $0$ and solving for $\\mathbf{\\hat\\beta}$.\n",
    "\n",
    "You should have obtained\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{\\hat\\beta} = (\\mathbf{Z}^\\top\\mathbf{Z})^{-1}\\mathbf{Z}^\\top\\mathbf{y} \\hspace{1.5cm} (1)\n",
    "\\end{equation}\n",
    "(or equivalent).\n",
    "\n",
    "> **Task 3**: Implement OLS in Python, to obtain a vector of coefficients. Hint: Since Python 3.6 you can use the ```@``` symbol for matrix multiplication. Note the ```inv``` function, as imported in the code provided. Print out the coefficients using the ```print``` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature creation\n",
    "\n",
    "def phi(X, degree):\n",
    "    N,D = X.shape\n",
    "    for d in range(2,degree+1):\n",
    "        X = np.column_stack([X,X[:,0:D]**d])\n",
    "    X = np.column_stack([np.ones(len(X)), X])\n",
    "    return X\n",
    "\n",
    "# Polynomial degree\n",
    "degree = 2\n",
    "\n",
    "Z = phi(X,degree)\n",
    "\n",
    "#insert here your solution for Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ques 2**: What could go wrong in building this model? Specifically, wrt Eq. (1) ?\n",
    "\n",
    "## Evaluation and Comparison\n",
    "\n",
    "Before deploying a model, we should evaluate it to know how useful it is. For this purpose we have held aside some *test data*. It is important to conduct an identical preprocessing on the test data as the train data; noting that the training procedure is done in complete ignorance of the test data.\n",
    "\n",
    "> **Task 4**: Load and preprocess the test data (```data_test.csv```). Use different variable names (for example, ```X_test``` and ```y_test```)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here your solution for Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the quality of predictions. For this task we use the *mean squared error* as the loss metric, over $N$ examples as\n",
    "\\begin{equation}\n",
    "\t\\textsf{MSE}(\\mathbf{\\hat\\beta}) = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2  \\hspace{1.5cm} (2)\n",
    "\\end{equation}\n",
    "\n",
    "> **Task 5**: Implement MSE, i.e., a function such that ```mse = MSE(y_test,y_pred)```. Print out the value obtained. Hints: in ```Numpy``` the number of examples in a vector can be obtained by the ```len``` function; and you can make use of ```mean``` again.\n",
    "\n",
    "> **Task 6**: Additionally, calculate the MSE where all predictions for test examples are simply the mean of $Y$ in the training data (i.e., a *baseline*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here your solutions for Tasks 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you should obtain something like the following ($w$ values have been rounded - $w$ for $\\mathbf{\\hat\\beta}$ here).\n",
    "\n",
    "        w =  [ 1.902  0.117  0.691  0.031 -0.676  0.24   0.031]\n",
    "        MSE on test data   0.546191331548\n",
    "        MSE baseline       1.7691746529\n",
    "          \n",
    "> **Task 7**: What is the change to MSE if we replace the outlier with a different value (e.g., $0$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here your solution for Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ques 3**: Outliers can have a powerful influence on model accuracy. Explain this influence with regrard to Eq (2)\n",
    "\n",
    "Why is it important to use a test set for evaluation and not just simply measure the MSE on the training data that we had originally? The answer is to avoid *overfitting*. Overfitting can be demonstrated by comparing results on the training and test sets for different parameter configurations. Here we can see what happens when we vary the degree of the polynomial expansion:\n",
    "\n",
    "![Overfitting](fig/fig2.png)\n",
    "\n",
    "> **Ques 4**: What degree of polynomial should you use in your final deployed model, according to this plot?\n",
    "\n",
    "> **Task 8** (Bonus): Reproduce the results of the figure above. Hints: put the test procedure in a loop, incrementing the degree of polynomial in the expansion each time, then use plotting functions as demonstrated in the earlier tasks; at each iteration of the loop you will need to obtain predictions on both the training and test data using your ```MSE``` function.}\n",
    "\n",
    "> **Task 9** (Bonus): Apply the regression task using only the two principal components (i.e., apply PCA before the regression). You may implement PCA yourself or use ```sklearn.decomposition.PCA``` from the Scikit-Learn framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here your solution for task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#insert here your solution for task 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
